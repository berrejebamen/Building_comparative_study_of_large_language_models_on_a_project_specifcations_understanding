{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf76665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "What is etl\n",
      "\n",
      "Predicted Answer:\n",
      "A data integration process that combines data from multiple data sources into a single on , the consistent data store that is loaded into a data warehouse or other target system\n",
      "\n",
      "Cleaned Answer:\n",
      "A data integration process that combines data from multiple data sources into a single on , the consistent data store that is loaded into a data warehouse or other target system\n",
      "\n",
      "True Answer:\n",
      "Hot file , amex file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "def get_longest_word(text):\n",
    "    words = [word for word in text.split() if word.strip()]  # Filter out spaces and empty strings\n",
    "    return max(words, key=len) if words else \"\"\n",
    "def get_second_largest_word(text):\n",
    "    words = [word for word in text.split() if word.strip()]  # Filter out spaces and empty strings\n",
    "    sorted_words = sorted(words, key=len, reverse=True)\n",
    "    return sorted_words[1] if len(sorted_words) >= 2 else \"\"\n",
    "def exact_match_score(pred_answer, true_answer):\n",
    "    pred_answer = pred_answer.lower()\n",
    "    true_answer = true_answer.lower()\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    # Check if the length of pred_answer equals the true_answer\n",
    "    if len(pred_answer) == len(true_answer):\n",
    "        score += 0.1\n",
    "\n",
    "    # Check if the first and last letter of pred_answer equal the true_answer\n",
    "    if pred_answer and pred_answer[0] == true_answer[0]:  #or pred_answer[-1] == true_answer[-1]:\n",
    "        score += 0.3\n",
    "\n",
    "    # Check if the longest word in pred_answer and true_answer are equal\n",
    "    pred_longest_word = get_longest_word(pred_answer)\n",
    "    true_longest_word = get_longest_word(true_answer)\n",
    "    if pred_longest_word == true_longest_word:\n",
    "        score += 0.3\n",
    "\n",
    "    # Check if pred_answer exactly matches the true_answer\n",
    "    if pred_answer == true_answer:\n",
    "        score = 1.0\n",
    "\n",
    "    return score\n",
    "\n",
    "def remove_extra_chars(text):\n",
    "    # Remove '##' characters\n",
    "    text = text.replace(\" ##\", \"\").replace(\"##\", \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(question, text, true_answer, model_path=None):\n",
    "    if model_path:\n",
    "        os.chdir(model_path)\n",
    "\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    input_ids = tokenizer.encode(question, text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    num_seg_a = sep_idx + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "\n",
    "    start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "    answer_start = torch.argmax(start_logits)\n",
    "    answer_end = torch.argmax(end_logits)\n",
    "\n",
    "    pred_answer = \" \".join(tokens[answer_start:answer_end + 1])\n",
    "    correct_answer=remove_extra_chars(pred_answer)\n",
    "    em_score = exact_match_score(correct_answer, pred_answer)\n",
    "    \n",
    "    return pred_answer, em_score\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Rest of the code...\n",
    "if __name__ == '__main__':\n",
    "    question = 'what is ETL'\n",
    "    with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    true_answer =\"Hot file , amex file\"\n",
    "    model_path = ''  # Adjust the model path if needed\n",
    "    \n",
    "    pred_answer, em_score = evaluate(question, text, true_answer, model_path)\n",
    "\n",
    "    print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "    print(\"\\nPredicted Answer:\\n{}\".format(pred_answer.capitalize()))\n",
    "    print(\"\\nCleaned Answer:\\n{}\".format(remove_extra_chars(pred_answer.capitalize())))\n",
    "    print(\"\\nTrue Answer:\\n{}\".format(true_answer.capitalize()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cc81c",
   "metadata": {},
   "source": [
    "# Testing and Preparing metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "148c89e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "What is  example of hot files\n",
      "\n",
      "Predicted Answer:\n",
      "Hot file , am ##ex file\n",
      "\n",
      "Cleaned Answer:\n",
      "Hot file , amex file\n",
      "\n",
      "True Answer:\n",
      "Hot file , amex file\n",
      "Predicted answer: Hot file , amex file\n",
      "True answer: Hot file , amex file\n",
      "Exact Match (EM) score: 1.0\n",
      "F1 score: 0.9999999925\n",
      "BLEU score: 1.0\n",
      "ROUGE-2 score(F1_score): 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model_evaluation\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "from nltk.translate.bleu_score import sentence_bleu ,SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def get_longest_word(text):\n",
    "    words = [word for word in text.split() if word.strip()]  # Filter out spaces and empty strings\n",
    "    return max(words, key=len) if words else \"\"\n",
    "def get_second_largest_word(text):\n",
    "    words = [word for word in text.split() if word.strip()]  # Filter out spaces and empty strings\n",
    "    sorted_words = sorted(words, key=len, reverse=True)\n",
    "    return sorted_words[1] if len(sorted_words) >= 2 else \"\"\n",
    "def exact_match_score(pred_answer, true_answer):\n",
    "    pred_answer = pred_answer.lower()\n",
    "    true_answer = true_answer.lower()\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    # Check if the length of pred_answer equals the true_answer\n",
    "    if len(pred_answer) == len(true_answer):\n",
    "        score += 0.1\n",
    "\n",
    "    # Check if the first and last letter of pred_answer equal the true_answer\n",
    "    if pred_answer and pred_answer[0] == true_answer[0]  or pred_answer[-1] == true_answer[-1]:\n",
    "        score += 0.1\n",
    "\n",
    "    # Check if the longest word in pred_answer and true_answer are equal\n",
    "    pred_longest_word = get_longest_word(pred_answer)\n",
    "    true_longest_word = get_longest_word(true_answer)\n",
    "    if pred_longest_word == true_longest_word:\n",
    "        score += 0.25\n",
    "    pred_longest_second_word = get_second_largest_word(pred_answer)\n",
    "    true_longest_second_word = get_second_largest_word(true_answer)\n",
    "    if pred_longest_second_word == true_longest_second_word:\n",
    "        score += 0.25\n",
    "    # Check if pred_answer exactly matches the true_answer\n",
    "    if pred_answer == true_answer:\n",
    "        score = 1.0\n",
    "\n",
    "    return score\n",
    "\n",
    "def calculate_bleu_score(reference, hypothesis):\n",
    "    # Tokenize the reference and hypothesis translations\n",
    "    reference_tokens = nltk.word_tokenize(reference.lower())\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis.lower())\n",
    "\n",
    "    # Calculate BLEU score using NLTK's corpus BLEU implementation\n",
    "    # We use weights=(1, 0, 0, 0) for unigram precision (BLEU-1)\n",
    "    bleu_score = nltk.translate.bleu_score.sentence_bleu([reference_tokens], hypothesis_tokens, weights=(1, 0, 0, 0))\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_f1(predicted_answer, true_answer):\n",
    "    predicted_tokens = set(predicted_answer.lower().split())\n",
    "    true_tokens = set(true_answer.lower().split())\n",
    "    common_tokens = predicted_tokens.intersection(true_tokens)\n",
    "    precision = len(common_tokens) / (len(predicted_tokens) + 1e-8)\n",
    "    recall = len(common_tokens) / (len(true_tokens) + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "    return f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Rest of the code...\n",
    "if __name__ == '__main__':\n",
    "    question = 'what is  example of hot files'\n",
    "    with open('data.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    true_answer =\"Hot file , amex file\"\n",
    "    model_path = ''  # Adjust the model path if needed\n",
    "    \n",
    "    pred_answer, em_score = evaluate(question, text, true_answer, model_path)\n",
    "\n",
    "    print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "    print(\"\\nPredicted Answer:\\n{}\".format(pred_answer.capitalize()))\n",
    "    print(\"\\nCleaned Answer:\\n{}\".format(remove_extra_chars(pred_answer.capitalize())))\n",
    "    print(\"\\nTrue Answer:\\n{}\".format(true_answer.capitalize()))\n",
    "    \n",
    "\n",
    "def calculate_rouge2_score(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge2'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores['rouge2'].fmeasure\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "em_score = exact_match_score(remove_extra_chars(pred_answer.capitalize()), true_answer.capitalize())\n",
    "f1_score = compute_f1(remove_extra_chars(pred_answer.capitalize()), true_answer.capitalize())\n",
    "bleu_score = calculate_bleu_score(remove_extra_chars(pred_answer.capitalize()), true_answer.capitalize())\n",
    "rouge2_score = calculate_rouge2_score(remove_extra_chars(pred_answer.capitalize()), true_answer.capitalize())\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Predicted answer: {remove_extra_chars(pred_answer.capitalize())}\")\n",
    "print(f\"True answer: {true_answer.capitalize()}\")\n",
    "print(f\"Exact Match (EM) score: {em_score}\")\n",
    "print(f\"F1 score: {f1_score}\")\n",
    "print(f\"BLEU score: {bleu_score}\")\n",
    "print(f\"ROUGE-2 score(F1_score): {rouge2_score}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36af0414",
   "metadata": {},
   "source": [
    "# BERT BASED BOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad86cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your question (type 'exit' to end): how the categorized disputes are uploaded into the database ?\n",
      "Enter the true answer for evaluation (type 'skip' to skip evaluation): the categorized disputes are uploaded into the database at specified intervals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "How the categorized disputes are uploaded into the database ?\n",
      "\n",
      "Predicted Answer:\n",
      "At specified intervals\n",
      "\n",
      "True Answer:\n",
      "The categorized disputes are uploaded into the database at specified intervals\n",
      "BLEU score: 0.2727272727272727\n",
      "ROUGE-2 score(F1_score): 0.33333333333333337\n",
      "Results updated and saved.\n",
      "Enter your question (type 'exit' to end): exit\n"
     ]
    }
   ],
   "source": [
    "# Rest of the code...\n",
    "import pandas as pd\n",
    "from transformers import pipeline, logging\n",
    "import transformers\n",
    "def get_longest_word(text):\n",
    "    words = [word for word in text.split() if word.strip()]  # Filter out spaces and empty strings\n",
    "    return max(words, key=len) if words else \"\"\n",
    "def get_second_largest_word(text):\n",
    "    words = [word for word in text.split() if word.strip()]  # Filter out spaces and empty strings\n",
    "    sorted_words = sorted(words, key=len, reverse=True)\n",
    "    return sorted_words[1] if len(sorted_words) >= 2 else \"\"\n",
    "def exact_match_score(pred_answer, true_answer):\n",
    "    pred_answer = pred_answer.lower()\n",
    "    true_answer = true_answer.lower()\n",
    "\n",
    "    score = 0.0\n",
    "\n",
    "    # Check if the length of pred_answer equals the true_answer\n",
    "    if len(pred_answer) == len(true_answer):\n",
    "        score += 0.1\n",
    "\n",
    "    # Check if the first and last letter of pred_answer equal the true_answer\n",
    "    #if pred_answer and pred_answer[0] == true_answer[0]:  #or pred_answer[-1] == true_answer[-1]:\n",
    "        #score += 0.3\n",
    "\n",
    "    # Check if the longest word in pred_answer and true_answer are equal\n",
    "    pred_longest_word = get_longest_word(pred_answer)\n",
    "    true_longest_word = get_longest_word(true_answer)\n",
    "    if pred_longest_word == true_longest_word:\n",
    "        score += 0.3\n",
    "\n",
    "    # Check if pred_answer exactly matches the true_answer\n",
    "    if pred_answer == true_answer:\n",
    "        score = 1.0\n",
    "\n",
    "    return score\n",
    "def save_dataframe_to_csv(dataframe, file_path):\n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "def remove_extra_chars(text):\n",
    "    # Remove '##' characters\n",
    "    text = text.replace(\" ##\", \"\").replace(\"##\", \"\")\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  \n",
    "    return text\n",
    "def evaluate(question, text, true_answer, model_path=None):\n",
    "    if model_path:\n",
    "        os.chdir(model_path)\n",
    "\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    input_ids = tokenizer.encode(question, text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    num_seg_a = sep_idx + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "\n",
    "    start_logits, end_logits = outputs['start_logits'], outputs['end_logits']\n",
    "    answer_start = torch.argmax(start_logits)\n",
    "    answer_end = torch.argmax(end_logits)\n",
    "\n",
    "    # Get the predicted answer from the tokens\n",
    "    pred_answer = tokenizer.decode(input_ids[answer_start:answer_end + 1], skip_special_tokens=True)\n",
    "\n",
    "    # Clean the answer and the true answer\n",
    "    pred_answer = remove_extra_chars(pred_answer)\n",
    "    correct_answer=remove_extra_chars(pred_answer)\n",
    "\n",
    "    # Calculate the exact match score\n",
    "    em_score = exact_match_score(correct_answer, pred_answer)\n",
    "\n",
    "    return pred_answer, em_score\n",
    "def evaluate(question, text, true_answer, model_path=None):\n",
    "    if model_path:\n",
    "        os.chdir(model_path)\n",
    "\n",
    "    model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "    input_ids = tokenizer.encode(question, text, add_special_tokens=True, truncation=True, max_length=512)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "    num_seg_a = sep_idx + 1\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "\n",
    "    start_logits, end_logits = outputs['start_logits'], outputs['end_logits']\n",
    "    answer_start = torch.argmax(start_logits)\n",
    "    answer_end = torch.argmax(end_logits)\n",
    "\n",
    "    # Get the predicted answer from the tokens\n",
    "    pred_answer = tokenizer.decode(input_ids[answer_start:answer_end + 1], skip_special_tokens=True)\n",
    "\n",
    "    # Clean the answer and the true answer\n",
    "    pred_answer = remove_extra_chars(pred_answer)\n",
    "    correct_answer=remove_extra_chars(pred_answer)\n",
    "\n",
    "    # Calculate the exact match score\n",
    "    em_score = exact_match_score(correct_answer, pred_answer)\n",
    "\n",
    "    return pred_answer, em_score\n",
    "def load_existing_results(file_path):\n",
    "    try:\n",
    "        existing_df = pd.read_csv(file_path)\n",
    "        return existing_df\n",
    "    except FileNotFoundError:\n",
    "        return pd.DataFrame(columns=[\"Question\", \"True Answer\", \"Predicted Answer\", \"BLEU Score\", \"ROUGE-2 Score\"])\n",
    "\n",
    "def save_dataframe_to_csv(dataframe, file_path):\n",
    "    dataframe.to_csv(file_path, index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_path = ''  # Adjust the model path if needed\n",
    "    result_file_path = \"output1.csv\"  # Adjust the file path as needed\n",
    "    with open('data2.txt', 'r', encoding='utf-8') as file:\n",
    "        contexts = file.read()\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"Enter your question (type 'exit' to end): \")\n",
    "\n",
    "        if question.lower() == 'exit':\n",
    "            break\n",
    "\n",
    "        true_answer = input(\"Enter the true answer for evaluation (type 'skip' to skip evaluation): \")\n",
    "        if true_answer.lower() == 'skip':\n",
    "            true_answer = ''\n",
    "        pred_answer, em_score = evaluate(question, contexts, true_answer, model_path)\n",
    "        print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n",
    "        print(\"\\nPredicted Answer:\\n{}\".format(pred_answer.capitalize()))\n",
    "\n",
    "        if true_answer:\n",
    "            bleu_score = calculate_bleu_score(remove_extra_chars(pred_answer.capitalize()), true_answer.capitalize())\n",
    "            rouge2_score = calculate_rouge2_score(remove_extra_chars(pred_answer.capitalize()), true_answer.capitalize())\n",
    "            print(\"\\nTrue Answer:\\n{}\".format(true_answer.capitalize()))\n",
    "            print(f\"BLEU score: {bleu_score}\")\n",
    "            print(f\"ROUGE-2 score(F1_score): {rouge2_score}\")\n",
    "            \n",
    "            new_data = {\n",
    "                \"Question\": question.capitalize(),\n",
    "                \"Predicted Answer\": pred_answer.capitalize(),\n",
    "                \"True Answer\": true_answer.capitalize(),\n",
    "                \"BLEU Score\": bleu_score,\n",
    "                \"ROUGE-2 Score\": rouge2_score\n",
    "            }\n",
    "            existing_results_df = load_existing_results(result_file_path)\n",
    "            existing_results_df = pd.concat([existing_results_df, pd.DataFrame([new_data])], ignore_index=True)\n",
    "            save_dataframe_to_csv(existing_results_df, result_file_path)\n",
    "            \n",
    "            print(\"Results updated and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef44505f",
   "metadata": {},
   "source": [
    "# BERT FINAL RESULT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "588b9cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>ROUGE-2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is etl</td>\n",
       "      <td>A data integration process that combines data ...</td>\n",
       "      <td>Extract , transform , load</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does etl stand for</td>\n",
       "      <td>Extract, transform, and load</td>\n",
       "      <td>Extract , transform , load</td>\n",
       "      <td>0.578930</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is sla</td>\n",
       "      <td>Service - level agreement ( sla ) is a contrac...</td>\n",
       "      <td>S a contract between a service provider and it...</td>\n",
       "      <td>0.764060</td>\n",
       "      <td>0.760563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How the categorized disputes are uploaded into...</td>\n",
       "      <td>At specified intervals</td>\n",
       "      <td>The categorized disputes are uploaded into the...</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0                                       What is etl    \n",
       "1                            What does etl stand for   \n",
       "2                                        What is sla   \n",
       "3  How the categorized disputes are uploaded into...   \n",
       "\n",
       "                                    Predicted Answer  \\\n",
       "0  A data integration process that combines data ...   \n",
       "1                       Extract, transform, and load   \n",
       "2  Service - level agreement ( sla ) is a contrac...   \n",
       "3                             At specified intervals   \n",
       "\n",
       "                                         True Answer  BLEU Score  \\\n",
       "0                         Extract , transform , load    0.000308   \n",
       "1                         Extract , transform , load    0.578930   \n",
       "2  S a contract between a service provider and it...    0.764060   \n",
       "3  The categorized disputes are uploaded into the...    0.272727   \n",
       "\n",
       "   ROUGE-2 Score  \n",
       "0       0.000000  \n",
       "1       0.400000  \n",
       "2       0.760563  \n",
       "3       0.333333  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"output1.csv\") \n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271265f",
   "metadata": {},
   "source": [
    "# Randomly picking 10 rows from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0430911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>True Answer</th>\n",
       "      <th>Predicted Answer</th>\n",
       "      <th>BLEU Score</th>\n",
       "      <th>ROUGE-2 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>When do the status \"pending\" apply</td>\n",
       "      <td>When no settlement has been linked to a sale</td>\n",
       "      <td>When no settlement has been linked to a sale</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What is unit test in a clean architecture</td>\n",
       "      <td>In clean architecture, a unit test is a type o...</td>\n",
       "      <td>A unit test is a type of automated test that i...</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.810811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>What happened when when the capture status of ...</td>\n",
       "      <td>The \"n/a\" (not applicable) status is assigned</td>\n",
       "      <td>\"n/a\" status is assigned</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What format should the file name respect</td>\n",
       "      <td>Ax_merchant_number_epa_amex_$utc_date$_utc_time</td>\n",
       "      <td>Ax_merchant_number_epa_amex_$utc</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What is scaling plan</td>\n",
       "      <td>A scaling plan is a set of rules and costs ass...</td>\n",
       "      <td>A scaling plan is a set of rules and costs ass...</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.491228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Example of naming convention</td>\n",
       "      <td>Salesreport_{airlinecode}{salesreportid}{recep...</td>\n",
       "      <td>Salesreport_airlinecodesalesreportidreceptiondate</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is load test</td>\n",
       "      <td>Load test is the objective is to ensure that l...</td>\n",
       "      <td>Load test is the objective is to ensure that l...</td>\n",
       "      <td>0.481481</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is sla</td>\n",
       "      <td>Is a contract between a service provider and i...</td>\n",
       "      <td>Service-level agreement (sla) is a contract be...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>What does soc stand for</td>\n",
       "      <td>Summary of charge</td>\n",
       "      <td>Summary of charge</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What is the purpose of matching rules module</td>\n",
       "      <td>The matching rules module enables the user to ...</td>\n",
       "      <td>Matching rules module enables the user to chan...</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Question  \\\n",
       "21                 When do the status \"pending\" apply   \n",
       "13          What is unit test in a clean architecture   \n",
       "22  What happened when when the capture status of ...   \n",
       "9           What format should the file name respect    \n",
       "11                               What is scaling plan   \n",
       "14                       Example of naming convention   \n",
       "7                                   What is load test   \n",
       "1                                         What is sla   \n",
       "23                            What does soc stand for   \n",
       "18       What is the purpose of matching rules module   \n",
       "\n",
       "                                          True Answer  \\\n",
       "21       When no settlement has been linked to a sale   \n",
       "13  In clean architecture, a unit test is a type o...   \n",
       "22     The \"n/a\" (not applicable) status is assigned    \n",
       "9    Ax_merchant_number_epa_amex_$utc_date$_utc_time    \n",
       "11  A scaling plan is a set of rules and costs ass...   \n",
       "14  Salesreport_{airlinecode}{salesreportid}{recep...   \n",
       "7   Load test is the objective is to ensure that l...   \n",
       "1   Is a contract between a service provider and i...   \n",
       "23                                 Summary of charge    \n",
       "18  The matching rules module enables the user to ...   \n",
       "\n",
       "                                     Predicted Answer  BLEU Score  \\\n",
       "21       When no settlement has been linked to a sale    1.000000   \n",
       "13  A unit test is a type of automated test that i...    0.615385   \n",
       "22                           \"n/a\" status is assigned    0.545455   \n",
       "9                    Ax_merchant_number_epa_amex_$utc    0.400000   \n",
       "11  A scaling plan is a set of rules and costs ass...    0.319149   \n",
       "14  Salesreport_airlinecodesalesreportidreceptiondate    0.000000   \n",
       "7   Load test is the objective is to ensure that l...    0.481481   \n",
       "1   Service-level agreement (sla) is a contract be...    0.300000   \n",
       "23                                  Summary of charge    1.000000   \n",
       "18  Matching rules module enables the user to chan...    0.888889   \n",
       "\n",
       "    ROUGE-2 Score  \n",
       "21       1.000000  \n",
       "13       0.810811  \n",
       "22       0.545455  \n",
       "9        0.769231  \n",
       "11       0.491228  \n",
       "14       0.000000  \n",
       "7        0.631579  \n",
       "1        0.400000  \n",
       "23       1.000000  \n",
       "18       0.967742  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "df = pd.read_csv('output3.csv')\n",
    "\n",
    "# Randomly select five rows\n",
    "num_samples = 10\n",
    "random_indices = random.sample(range(len(df)), num_samples)\n",
    "random_rows = df.iloc[random_indices]\n",
    "\n",
    "random_rows.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
